{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d762d280",
   "metadata": {},
   "source": [
    "# HNLQ: Part-01\n",
    "\n",
    "This series of notebook demonstrates hierarchical nested lattice quantization using the D4 lattice, showing how multi-level quantization can improve rate-distortion performance.\n",
    "\n",
    "In part-01, we primarily check the methods, and see it side-by-side by with NLQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ece85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gemvq.quantizers.hnlq import HNLQ as HQ\n",
    "from gemvq.quantizers.utils import get_d4, closest_point_Dn, SIG_D4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893caa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801e800",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "of using hierarchical lattice quantization to quantize a 4D vector.\n",
    "\n",
    "We will take $x=(1.2,-0.7,2.4,0.3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757310ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m M = \u001b[32m1\u001b[39m\n\u001b[32m     10\u001b[39m config = HNLQConfig(q=q, beta=beta, alpha=alpha, eps=eps, M=M)\n\u001b[32m     11\u001b[39m hq = HQ(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     G=\u001b[43mG\u001b[49m,\n\u001b[32m     13\u001b[39m     Q_nn=closest_point_Dn,\n\u001b[32m     14\u001b[39m     config=config,\n\u001b[32m     15\u001b[39m     dither=np.zeros(\u001b[32m4\u001b[39m)\n\u001b[32m     16\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Create test sample and normalize it\u001b[39;00m\n\u001b[32m     20\u001b[39m test_sample = np.array([\u001b[32m1.2\u001b[39m, -\u001b[32m0.7\u001b[39m, \u001b[32m2.4\u001b[39m, \u001b[32m0.3\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(31)\n",
    "from gemvq.quantizers.hnlq import HNLQConfig\n",
    "\n",
    "q = 2\n",
    "beta = 1\n",
    "alpha = 1\n",
    "eps = 1e-8\n",
    "M = 1\n",
    "\n",
    "config = HNLQConfig(q=q, beta=beta, alpha=alpha, eps=eps, M=M)\n",
    "hq = HQ(\n",
    "    G=G,\n",
    "    Q_nn=closest_point_Dn,\n",
    "    config=config,\n",
    "    dither=np.zeros(4)\n",
    ")\n",
    "    \n",
    "\n",
    "# Create test sample and normalize it\n",
    "test_sample = np.array([1.2, -0.7, 2.4, 0.3])\n",
    "test_sample = test_sample/np.linalg.norm(test_sample)  # normalize to unit vector\n",
    "print(\"Original vector:\", test_sample)\n",
    "\n",
    "# Test quantization\n",
    "x_q = quantizer.quantize(test_sample, with_dither=False)\n",
    "print(\"Quantized vector:\", x_q)\n",
    "\n",
    "# Test encode/decode separately\n",
    "enc, T = quantizer.encode(test_sample, with_dither=False)\n",
    "x_dq = quantizer.decode(enc, T, with_dither=False)\n",
    "print(\"encoded:\", enc)\n",
    "print(\"overloading factor:\", T)\n",
    "print(\"Decoded vector:\", x_dq)\n",
    "\n",
    "# Print error\n",
    "error = np.linalg.norm(test_sample - x_dq)\n",
    "print(f\"Reconstruction Error (L2 norm): {error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503353ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "q = 2\n",
    "M = 1\n",
    "G = get_d4()\n",
    "Q_nn = lambda x: closest_point_Dn(x)  # D4 lattice closest point algorithm\n",
    "beta = 1\n",
    "\n",
    "test_sample = np.array([1.2, -0.7, 2.4, 0.3])\n",
    "\n",
    "print(\"Original Sample:\", np.round(test_sample, 3))\n",
    "\n",
    "quantizer = HNLQ(\n",
    "            G=G,\n",
    "            Q_nn=Q_nn,\n",
    "            q=q,\n",
    "            beta=beta,\n",
    "            alpha=1.0,\n",
    "            M=M,\n",
    "            eps=np.zeros(d),\n",
    "            dither=np.zeros(d)\n",
    "        )\n",
    "enc, T = quantizer.encode(test_sample, with_dither=False)\n",
    "print(f\"\\t encoded (q={q}, M={M},T={T})\")\n",
    "xh = quantizer.decode(enc,T, with_dither=False)\n",
    "print(f\"\\t bit vector at m=0 is {enc[0]})\")\n",
    "print(f\"\\t bit vector at m=1 is {enc[1]})\")\n",
    "error = np.linalg.norm(test_sample - xh)\n",
    "print(\"\\t Reconstructed Sample:\", np.round(xh, 3))\n",
    "print(f\"Reconstruction Error (L2 norm): {error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92163dc",
   "metadata": {},
   "source": [
    "## Effect of q and M\n",
    "\n",
    "Let's analyze how different values of q and number of levels M affect the quantization of a single sample:\n",
    "- Quantization factors (q): 2, 4, 8\n",
    "- Number of levels (M): 1, 2, 3, 4\n",
    "\n",
    "This will help understand:\n",
    "1. The impact of quantization factor on precision\n",
    "2. How additional levels improve reconstruction\n",
    "3. Trade-offs between q values and number of levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single test sample\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "beta = SIG_D4\n",
    "G = get_d4()\n",
    "Q_nn = lambda x: closest_point_Dn(x)  # D4 lattice closest point algorithm\n",
    "\n",
    "# test_sample = np.random.normal(0, 1.0, size=d)\n",
    "# sample from uniform distribution\n",
    "\n",
    "def run_quantization(q, M, test_sample):\n",
    "    for m in range(1,M+1):\n",
    "        quantizer = HNLQ(\n",
    "                    G=G,\n",
    "                    Q_nn=Q_nn,\n",
    "                    q=q,\n",
    "                    beta=1,\n",
    "                    alpha=1.0,\n",
    "                    M=m,\n",
    "                    eps=np.zeros(4),\n",
    "                    dither=np.zeros(4)\n",
    "                )\n",
    "        enc, T = quantizer.encode(test_sample, with_dither=False)\n",
    "        xh = quantizer.decode(enc,T, with_dither=False)\n",
    "        error = np.linalg.norm(test_sample - xh)\n",
    "        print(f\"\\t Depth x: {m}\")\n",
    "        print(f\"\\t De-quantized x: {xh}\")\n",
    "        print(f\"\\t Error (L2 norm): {error:.2f}\")\n",
    "        print(f\"\\t Overload Factor : {T}\")\n",
    "\n",
    "\n",
    "q = 4\n",
    "M = 5\n",
    "scale = q**(0)\n",
    "\n",
    "test_sample = scale*np.random.uniform(-1,1,size=d)\n",
    "print(\"Original Sample:\", np.round(test_sample, 3))\n",
    "print(f\" q = {q}, M= {M}: {error:.6f}\")\n",
    "run_quantization(4, 5, test_sample)\n",
    "\n",
    "q = 4\n",
    "M = 5\n",
    "scale = q**(M)\n",
    "\n",
    "test_sample = scale*np.random.uniform(-1,1,size=d)\n",
    "print(\"Original Sample:\", np.round(test_sample, 3))\n",
    "print(f\" q = {q}, M= {M}: {error:.6f}\")\n",
    "run_quantization(4, 5, test_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a972a",
   "metadata": {},
   "source": [
    "In the first case, when the expected norm of the input vector is small, for a given q, increasing M beyond a certain point (depth 1) does not significantly improve the quantization error. This is because the input vector is already well within the range that can be effectively quantized with the given q and M. Adding depth wastes bits without much gain. You can notice that the overload factor T is 1, indicating no overload distortion. Adding depth beyond what is necessary does not help.\n",
    "\n",
    "In the second case, when the expected norm of the input vector is larger, increasing M continues to reduce the quantization error. This is because the larger input values benefit from the additional levels of quantization, allowing for finer granularity in representing the input vector. \n",
    "\n",
    "An analogy with bitplane coding might help. For small values, the higher bitplanes (more significant bits) are not needed, so adding more levels (M) doesn't help much. But for larger values, having more bitplanes allows for a more accurate representation, thus reducing error.\n",
    "\n",
    "But does increasing q help? Yes, increasing q increases the range of values that can be represented at each level, which can also help reduce quantization error, especially for larger input values. However, this comes at the cost of increased complexity and potentially higher rate (more bits needed to represent the quantized values). Let us see this in action.\n",
    "\n",
    "Note that under the Hierarchical Lattice Quantization framework, the overload factor T indicates how many times the input vector exceeds the quantization range. A T value of 1 means the input is within the range, while higher values indicate overload distortion. This is crucial for understanding the effectiveness of the quantization process, especially when dealing with larger input values or higher dimensions.\n",
    "\n",
    "Also notice that under the Hierarchical Lattice Quantization framework, for given q, increasing M, he lattices are nested - creating fractal like structures. So, we can expect the error to be monotonically non-decreasing with increasing M. However, this nesting property does not hold when we vary q. For example, a (q=2, M=1) lattice is not necessarily nested within a (q=4, M=1) lattice. Hence, the error may not be monotonic when we vary q, but expect it to decrease on average with increasing q, at substantial increase in rate and computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f67312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to set alpha=0 to avoid scaling the lattice automatically\n",
    "# to see the effect of varying q and M on the quantization error for a fixed scale of input vectors.\n",
    "# but encoding routine gets into infinite loop. so we set alpha=1\n",
    "\n",
    "def evaluate_quantization(scale=1, q_values=[2,4,8], m_values=[1,2,3],N=1000):\n",
    "    er = np.zeros((len(q_values), len(m_values)))\n",
    "    for i,q in enumerate(q_values):\n",
    "        for j,m in enumerate(m_values):\n",
    "            \n",
    "            quantizer = HNLQ(\n",
    "                        G=G,\n",
    "                        Q_nn=Q_nn,\n",
    "                        q=q,\n",
    "                        beta=1,\n",
    "                        alpha=1,\n",
    "                        M=m,\n",
    "                        eps=np.zeros(4),\n",
    "                        dither=np.zeros(4) )\n",
    "            for n in range(N):\n",
    "                #test_sample = scale*np.random.uniform(-1,1,size=d)\n",
    "                test_sample = scale*np.random.normal(0,1,size=d)\n",
    "                enc, T = quantizer.encode(test_sample, with_dither=False)\n",
    "                xh = quantizer.decode(enc,T, with_dither=False)\n",
    "                error = np.linalg.norm(test_sample - xh)\n",
    "                er[i,j] += error\n",
    "            er[i,j] /= N\n",
    "            \n",
    "    df = pd.DataFrame(er, index=q_values, columns=m_values)\n",
    "    print(\"\\nSummary of Average Reconstruction Errors (L2 norm):\")\n",
    "    print(df)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i, q in enumerate(q_values):\n",
    "        plt.plot(m_values, er[i], marker='o', label=f'q={q}')\n",
    "    plt.xlabel('M values')\n",
    "    plt.ylabel('Average Reconstruction Error (L2 norm)')\n",
    "    plt.title(f'Quantization Error vs M values (scale={scale})')\n",
    "    plt.xticks(m_values)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return er\n",
    "\n",
    "# Try different M values\n",
    "results = evaluate_quantization(scale=1, q_values=[2,4,8], m_values=[1,2,3])\n",
    "\n",
    "# Try different scales\n",
    "results = evaluate_quantization(scale=2**3, q_values=[2,4,8], m_values=[1,2,3])\n",
    "\n",
    "# Try different scales\n",
    "results = evaluate_quantization(scale=2**8, q_values=[2,4,8], m_values=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c718acc",
   "metadata": {},
   "source": [
    "So, increasing q increases the codebook size, and therefore, the rate, but it also allows for finer quantization at each level, which can reduce distortion. When the norm of the input is large, the reduction in distortion from increasing q can be significant. However, when the norm of the input is small, the benefit of increasing q may be less pronounced.\n",
    "\n",
    "Both q and M affect the rate-distortion trade-off, but in different ways. Increasing q increases the codebook size and rate, while increasing M allows for finer quantization at the cost of complexity. The optimal choice of q and M depends on the specific application and the characteristics of the input data. Arbitrarily increasing either q or M will not always lead to better performance, and there are diminishing returns as these parameters are increased.\n",
    "\n",
    "So, the norm of the input vector plays a crucial role in determining the effectiveness of increasing M and q in hierarchical lattice quantization. In particular, for the target rate of $R=M \\log_2(q)$ per d-dimensions. if the norm is $q^M$, both increasing q and M can significantly reduce distortion. However, if the norm is much smaller than $q^M$, increasing M may have limited benefits, while increasing q can still help reduce distortion. However, monotonicity is not guaranteed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemvq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
