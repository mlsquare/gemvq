---
title: "HNLQ: Brief"
---

# Introduction

Matrix multiplication is a core operation in deep learning and large language models (LLMs). On modern hardware, the main bottleneck is often **memory bandwidth** rather than compute. Quantization reduces the size of stored/loaded numbers, thus lowering memory usage.  

A particularly elegant approach is **nested lattice quantization**. It leverages the algebraic and geometric structure of lattices to design efficient quantizers with small lookup tables (LUTs), enabling fast inference with minimal loss of accuracy.

This tutorial introduces the concept of nested lattice quantizers, explains hierarchical extensions that allow **high-rate quantization** with small LUTs, and shows how this method can be applied to approximate inner products and matrix multiplication.

---

# Lattices and Voronoi Quantization

A **lattice** $L \subset \mathbb{R}^d$ is a discrete additive subgroup generated by integer combinations of basis vectors:

$$
L = G \mathbb{Z}^d, \quad G \in \mathbb{R}^{d \times d}
$$

```{python}
```
The **nearest neighbor quantizer** associated with $L$ is

$$
Q_L(x) = \arg\min_{\lambda \in L} \| x - \lambda \|, \quad x \in \mathbb{R}^d
$$

The set of points mapping to the origin is the **Voronoi region**:

$$
V_L = \{ x \in \mathbb{R}^d : Q_L(x) = 0 \}
$$

The second moment of a lattice is

$$
\sigma^2(L) = \frac{1}{d} \, \mathbb{E}\|Z\|^2, \quad Z \sim \text{Uniform}(V_L)
$$

---

# Example: Square Lattice $L = \mathbb{Z}^2$

Here $G = I_2$, so the lattice points are all integer pairs.  

- The Voronoi cell $V_L$ is the unit square $[-\tfrac{1}{2}, \tfrac{1}{2})^2$.  
- Scaling by $q=3$ gives a sublattice $3\mathbb{Z}^2$.  
- The nested code $A_q = \mathbb{Z}^2 \cap (3V_L)$ consists of integer points in the $3 \times 3$ square centered at the origin.

So the codebook is:

$$
A_3 = \{ (-1,-1), (-1,0), (-1,1), (0,-1), (0,0), (0,1), (1,-1), (1,0), (1,1) \}
$$

Thatâ€™s $3^2 = 9$ codewords, matching rate $R = \log_2(3) \approx 1.585$ bits/dimension.

```{python}
import matplotlib.pyplot as plt
import numpy as np

A3 = [(i,j) for i in [-1,0,1] for j in [-1,0,1]]

plt.figure(figsize=(4,4))
plt.scatter([p[0] for p in A3], [p[1] for p in A3], s=80, label="Codewords")
plt.gca().set_aspect("equal", adjustable="box")
plt.axhline(0, linewidth=0.5)
plt.axvline(0, linewidth=0.5)
plt.grid(True, linestyle="--", alpha=0.5)
plt.title("Nested code $A_3$ in $\\mathbb{Z}^2$")
plt.legend()
plt.show()
```


::: callout-note
### Notation: $A_q$ vs. $A_2, A_3$ (root lattices)

In this tutorial, $A_q$ denotes the **nested lattice codebook**
$$
A_q \;=\; L \cap (q\,V_L),
$$
i.e., the set of lattice points of the base lattice $L$ that lie in the scaled Voronoi region $qV_L$.  
For our worked example with $$L=\mathbb{Z}^2$ and $q=3$$, this is the 9-point square codebook
$$
A_3=\{(-1,-1),(-1,0),(-1,1),(0,-1),(0,0),(0,1),(1,-1),(1,0),(1,1)\}.
$$

This **is not** the root-lattice notation $A_2, A_3$ (which refer to specific Coxeter/Root lattices).  
Here, subscripts on $A_q$ indicate the **nesting ratio** $q$ of the codebook, not a lattice family.
:::


---

# Nested Lattice Quantizers

Given $r \in \mathbb{N}$, the scaled lattice $rL$ is a sublattice of $L$. The **nested lattice code** (also called Voronoi code) is
$$
A_r = L \cap (r V_L).
$$

This corresponds to the quotient group:

$$
L / rL \cong (\mathbb{Z}/r\mathbb{Z})^d
$$
which admits efficient encoding and decoding.  

- The **quantization rate** is $R = \log_2 r$ bits per dimension.  
- Quantization error arises when $Q_L(x) \notin rV_L$, called **overload error**.  

Nested lattice quantizers are near-optimal for Gaussian sources and admit **lookup-table decoding**, but the LUT size scales as

$$
|\text{LUT}| = 2^{2dR}
$$
which quickly becomes infeasible for high-rate quantization.

---

# Hierarchical Nested Lattice Quantizers

To overcome the LUT explosion, Kaplan & Ordentlich (2025) propose **hierarchical nested lattice quantization**.  

The key idea:  
- Quantize each vector to $M$ layers with **nesting ratio** $q$.  
- Use a small LUT of size $2^{2dR/M}$, accessed $M^2$ times.  
- Achieve high overall rate $R = M \log_2 q$ without requiring exponential memory.

## Encoder (Algorithm 1)

Given $x \in \mathbb{R}^d$, lattice $L$, nesting ratio $q$, and depth $M$:

1. Initialize $\tilde{g} \gets x$.
2. For $m = 0,1,\dots,M-1$:
   - Quantize: $\tilde{g} \gets Q_L(\tilde{g})$.
   - Store coset index: 
     $$
     b_m = [ G^{-1} \tilde{g} ] \bmod q
     $$
   - Scale down: $\tilde{g} \gets \tilde{g}/q$.
3. Return encoding vectors $b_0, \dots, b_{M-1}$.

## Decoder (Algorithm 2)

Given encoding vectors $b_m$:

1. Initialize $\hat{x} \gets 0$.
2. For $m = 0,\dots,M-1$:
   - Compute partial reconstruction
     $$
     x_m = G b_m - q \cdot Q_L \Big( \frac{G b_m}{q} \Big)
     $$
   - Accumulate: $\hat{x} \gets \hat{x} + q^m x_m$.
3. Return $\hat{x} \in L$.

**Guarantee:** $\hat{x} = Q_L(x)$ if and only if $Q^{\circ M}(x) = 0$ (no overload).

---

# Properties

- **Rate:** $R = M \log_2 q$ bits per dimension.  
- **Constellation:** 
  $$
  C_{L,q,M} = \sum_{m=0}^{M-1} q^m A_q
  $$
- **Overload event:** occurs when $Q^{\circ M}(x) \neq 0$.  
- **Successive refinement:** decoding fewer layers gives a coarse approximation.  

### Bounds

The codebook satisfies the sandwich bound:

$$
A_{q^M}(1 - r_{q,M}) \subseteq C_{L,q,M} \subseteq A_{q^M}(1 + r_{q,M})
$$

where

$$
r_{q,M} = \frac{1 - q^{1-M}}{q-1}
$$

This shows that hierarchical quantization is close in performance to a single Voronoi code with $r = q^M$.

---

# Worked Example ($L=\mathbb{Z}^2$, $q=3$, $M=2$)

We now encode, decode, and compute inner products step by step.

## Encoding Example

Let $x = (2.7, -1.2)$.

1. **Step 0:** Quantize to $\mathbb{Z}^2$:
   $$
   Q_{\mathbb{Z}^2}(x) = (3,-1)
   $$
   Coset index $b_0 = (0,2)$ mod $3$.  
   Scale down: $\tilde{g} = (1,-0.333)$.

2. **Step 1:** Quantize again:
   $$
   Q_{\mathbb{Z}^2}(\tilde{g}) = (1,0)
   $$
   Coset index $b_1 = (1,0)$ mod $3$.  
   Scale down again: $\tilde{g} = (0.333,0)$.

Since $M=2$, stop. The encoding is

$$
(b_0, b_1) = \big( (0,2), (1,0) \big)
$$

## Decoding Example

From $b_0,b_1$, reconstruct:
$$
\hat{x} = (Q_0 + 3 \cdot Q_1),
$$
where $Q_m$ maps indices back to $A_3$.  

Here:

- $b_0 = (0,2) \mapsto (0,-1)$,  
- $b_1 = (1,0) \mapsto (1,0)$.  

So
$$
\hat{x} = (0,-1) + 3 \cdot (1,0) = (3,-1).
$$

This matches $Q_{\mathbb{Z}^2}(x)$, i.e. the nearest lattice point.

---

# Fast Inner Product with Hierarchical Quantization

Suppose we quantize $x,y \in \mathbb{R}^2$ with $q=3,M=2$.  

Each decomposes as:

$$
\hat{x} = \hat{x}_0 + 3 \hat{x}_1, \quad \hat{y} = \hat{y}_0 + 3 \hat{y}_1, \quad \hat{x}_i, \hat{y}_j \in A_3
$$

Then

$$
\hat{x}^\top \hat{y} = \sum_{i=0}^1 \sum_{j=0}^1 3^{i+j} \cdot L(b_i(x), b_j(y))
$$
where $L(\cdot,\cdot)$ is a precomputed LUT of inner products in $A_3 \times A_3$.  

- **LUT size:** $|A_3|^2 = 81$.  
- **Number of lookups:** $M^2 = 4$.  

This makes inner products efficient while retaining high quantization rate.

```{python}
# Example: inner product via LUT
import numpy as np

A3 = [(-1,-1), (-1,0), (-1,1),
      (0,-1),  (0,0),  (0,1),
      (1,-1),  (1,0),  (1,1)]
A3 = np.array(A3)
LUT = {(tuple(a),tuple(b)): np.dot(a,b) for a in A3 for b in A3}

x0, x1 = (0,-1), (1,0)   # from the decoding example
y0, y1 = (1,1), (0,-1)   # an arbitrary second vector's layers

ip = (LUT[(x0,y0)]
      + 3*LUT[(x0,y1)]
      + 3*LUT[(x1,y0)]
      + 9*LUT[(x1,y1)])
print("Approximate inner product:", ip)
```

---

# Visualizing the Constellation

```{python}
# Constellation for q=3, M=2
import matplotlib.pyplot as plt

A3 = [(-1,-1), (-1,0), (-1,1),
      (0,-1),  (0,0),  (0,1),
      (1,-1),  (1,0),  (1,1)]
C = []
for x0 in A3:
    for x1 in A3:
        C.append((x0[0] + 3*x1[0], x0[1] + 3*x1[1]))

plt.figure(figsize=(5,5))
plt.scatter([c[0] for c in C], [c[1] for c in C], s=20)
plt.title("Constellation $C_{\\mathbb{Z}^2,3,2}$")
plt.axhline(0,linewidth=0.5)
plt.axvline(0,linewidth=0.5)
plt.gca().set_aspect("equal","box")
plt.grid(True, linestyle="--", alpha=0.5)
plt.show()
```

---

# Practical Notes

- **Scaling and dithering:** apply a scale $\beta$ and a dither $z \in V_L$ to balance granular error vs overload probability.  
- **Overload avoidance:** adaptively increase $\beta$ (e.g., $\beta \leftarrow 2^\alpha \beta$) until overload vanishes, and send the count as side information.  
- **One-sided quantization:** if only $x$ is quantized, precompute lookups $L_y(b_i(x))$ for speed.  
- **Product codes:** for $n \gg d$, split vectors into $K=n/d$ chunks and quantize each chunk independently.  
- **Universality:** randomized rotations make performance depend mainly on $\|x\|$, $\|y\|$, and $|x^\top y|$.

---

# Summary

- Standard nested lattice quantizers use LUTs of size $2^{2dR}$, too large for high rates.  
- Hierarchical quantization reduces LUT size to $2^{2dR/M}$, at the cost of $M^2$ lookups.  
- Worked examples with $L=\mathbb{Z}^2, q=3, M=2$ show how encoding, decoding, and inner product computation work in practice.  
- The scheme is both theoretically grounded and practically efficient.  

# References

- Kaplan, I., & Ordentlich, O. (2025). *High-Rate Nested-Lattice Quantized Matrix Multiplication with Small Lookup Tables*. arXiv:2505.13164.  
- Zamir, R. (2014). *Lattice Coding for Signals and Networks*. Cambridge University Press.  
